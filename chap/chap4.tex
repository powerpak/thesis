%% A chapter for my PhD dissertation
%% First author: Theodore Pak
%%
%% Must be included from main.tex.

\chapter{The PathogenDB software suite for genomic clinical microbiology \& epidemiology}
\label{chap:pathogendb}

\newcommand{\pathogendbpipeline}{Pa\-tho\-genDB-\allowbreak pipe\-line}
\newcommand{\pathogendbcomparison}{Pa\-tho\-genDB-\allowbreak com\-pa\-ri\-son}
\newcommand{\pathogendbviz}{Pa\-tho\-genDB-\allowbreak viz}

\sidequote{\emph{Jurassic Park}}{
  \speaker{John Hammond:} Dennis, our lives are in your hands and you have butterfingers?
  
  \speaker{Dennis Nedry:} \emph{[laughs]} I am totally unappreciated in my time. You can run this whole park from this room with minimal staff for up to three days. You think that kind of automation is easy? Or cheap? You know anybody who can network eight connection machines and debug two million lines of code for what I bid for this job?
}

\sidequote{\smallcaps{HAL}\oldstylenumbers{9000}, \emph{\oldstylenumbers{2001}: A Space Odyssey}}{
  Let me put it this way, Mr.\ Amor. The 9000 series is the most reliable computer ever made. No 9000 computer has ever made a mistake or distorted information. We are all, by any practical definition of the words, foolproof and incapable of error.
}

\begin{quote}
\emph{Next-generation sequencing (NGS) technologies have reduced the cost of acquiring genomic data from active infections in hospitals, with the potential to rapidly characterize patient-to-patient transmission with extreme precision. However, there is no integrated software solution for converting NGS data into species identifications, phylogenies, and drug susceptibilities, with particularly few options for handling \emph{de novo} assemblies. A clinical application would ideally provide a unified pipeline that runs semi-automated analyses to inform infection control interventions. We developed a modular open-source software suite called PathogenDB that implements major functionalities needed for genomic clinical microbiology and pathogen surveillance. A central laboratory information management system runs on a standard open-source Linux/Apache/MySQL/PHP stack. A modular genomics workflow, \pathogendbpipeline, automates de novo assembly, circularization, gene annotation, QC, epigenetic motif prediction. A comparative genomics module, \pathogendbcomparison, performs semi-automated phylogenetic analysis. Finally, a visualization suite, \pathogendbviz, integrates phylogenies and epidemiological data into a ``live view'' of putative transmissions mapped to hospital locations. Thus far, \pathogendbpipeline{} has been used to assemble and annotate 593 genomes from 7 species, and runs in <12 hours end-to-end. At an urban tertiary-care hospital, \pathogendbcomparison{} has genomically characterized one MRSA outbreak, two transmissions via solid organ transplant, and pseudo-outbreaks of \emph{S. maltophilia} and \emph{B. cepacia}. All three software packages are freely available on GitHub.
}
\end{quote}

There is increasing consensus that next-generation sequencing (NGS) technologies will eventually become mainstream equipment in clinical microbiology laboratories,\sidecite[-3cm]{Didelot2012,Harris2013,Joensen2014} considering that its nominal reagent cost is already within range of routine tests (\$25 per isolate for certain short-read technologies) and that it is likely to improve turnaround time and sample throughput for epidemiological investigations and hard-to-culture organisms.\autocite{Didelot2012,Koser2012} One significant barrier to this, as noted in Chapter \ref{chap:intro}, is that informatics and software infrastructure for the new diagnostic workflows afforded by these technologies are not yet widely available. While robotic culturing systems like Vitek and BD Phoenix include mature software packages for automating the executing and interpretation of routine tests, even integrating directly with standard laboratory information management systems (LIMS) so that results can be associated with patient metadata and sent directly back to ordering physicians via the electronic medical record (EMR), no such framework exists for genomic clinical microbiology.

We have already noted in Tables \ref{tab:id_bioinf_dbs} and \ref{tab:id_bioinf_tools} and in Chapter \ref{chap:intro} that many open source software packages exist for \emph{individual components} of such a pipeline but no end-to-end solution has yet been assembled by the research community. The most mature components that are currently available are typically steps closest to the sequencer, since the relatively small number of sequencing platforms and ubiquitous demand for certain invariant processing steps for their direct outputs (e.g., debarcoding and demultiplexing, filtering reads, quality control, alignment to a reference) have spurred researchers to create consensus solutions for them. Less mature are the steps involved in \emph{de novo} assembly and beyond this frontier, since the capability to finish assemblies without human intervention has only recently emerged\autocite{Bashir2012} and the underlying algorithms and heuristics are still an area of active research.\autocite{Sohn2016} Finishing, annotating, and comparing brand new bacterial assemblies, therefore, has previously been rare enough that a standard distributable ``toolkit'' was not needed. However, as long-read sequencing continues to drop in price and complexity\footnote{The MinION, by Oxford Nanopore Technologies, is available in \$1,000 starter kits, plugs into a USB port, and produces reads up to 10kbp; see \textcite{Check2014}} and \emph{de novo} assembly for these platforms becomes more accessible, demand for pathways bringing these data into medical microbiology is bound to rise.\autocite{Judge2016}

Fortunately, the construction of new pipelines built around existing smaller tools is a typical task in bioinformatics—so common, in fact, that meta-tools are available for this very purpose.\autocite{Koster2012,Goecks2010,Jamil2013} The fact that a standard pipeline has been slow to emerge should not be considered a bad omen; in fact, this same situation existed for human genomes when short-read NGS technologies first emerged. As large-scale community resource projects like 1000 Genomes were launched to take advantage of NGS, efforts to develop standardized pipelines to execute those projects naturally followed and bore fruit. The landmark publication for 1000 Genomes was filed in 2010,\autocite{Durbin2010} and within the few short years that followed, the tools that were developed to enable the project's goals were released,\autocite{McKenna2010} refined into best practices for the community,\autocite{VanderAuwera2013} and reached sufficient maturity that they could be validated and adopted for use in clinical whole genome and whole exome sequencing pipelines.\autocite{Linderman2014} These tools, particularly Broad's Genome Analysis Toolkit (GATK), took what had previously been a hodgepodge of nascent data formats and scripts and unified them into a library of a consistent design and a uniform language (Java), optimized them for re-use in diverse and parallelized computing environments, and most importantly, underwent battle-testing in enough real-world use cases to become de-facto standards. Given the rapid pace at which this occurred, we remain optimistic that similar widescale efforts in genomic microbiology can drive demand and community support for a similar toolkit for pathogens, and that such software's adoption can reach clinical laboratories within years of its initial release.\autocite{Pak2015}

Of course, there are many challenges that can be expected, although they generally fall into the category of engineering problems and can therefore leverage many of the lessons and algorithms already unearthed by human genomicists. The premise of a genomic microbiology pipeline gaining clinical use requires it to be fast, if not faster than substitutable lab tests like culturing.\autocite{Koser2012} Since this implies a turnaround time of one or two days, the software must be efficient. Ideally, it would not require the supercomputing power typically associated with human genomics, and instead run on a single server or a desktop machine that any microbiology lab could afford.\footnote{Alternatively, labs can indeed ``rent'' supercomputing power from cloud platforms like Amazon's Elastic Compute Cloud, but this introduces special complexities of its own.} Thankfully, bacterial genomic data are generally much smaller than their human counterparts, and many of the components in Table \ref{tab:id_bioinf_tools} were already developed for use on everyday desktop hardware. The software must be generally reliable, but seeing as no bioinformatics pipeline succeeds in real-world usage 100\% of the time (imagine contaminated DNA entering an analysis, or the computer running out of disk space), if it must fail, it should fail obviously and provide some clues as to where and why. If possible, it should save work up to the failed step so if analysis needs to be re-run, time does not have to be wasted re-running steps that previously succeeded. Perhaps most importantly, all steps need to be reproducible, because it would be extremely difficult (if not untenable on its face) to achieve diagnostic validity with a nondeterministic and therefore unauditable procedure. While this seems simple in principle for a computer program, in practice, with constantly changing sources of ``truth'' (such as changes in local and remote databases, updates to ancillary libraries, and evolving storage formats) this can be a devilishly complicated affair.

There are a few recent examples of publicly released, self-contained pipe\-lines that solve specific problems in clinical microbiology with NGS data. One is SURPI, which processes millions of reads from a metagenomic sample to rapidly search for evidence of pathogen DNA.\autocite{Naccache2014} Most notably, this pipeline was used to deliver a timely diagnosis for a case of neuroleptospirosis that eluded traditional diagnostic assays.\autocite{Wilson2014} It can be deployed on both standalone servers and cloud computing environments.\autocite{Naccache2014} Another is Mykrobe, which similarly processes NGS read files directly to generate clinican-friendly reports of predicted antimicrobial resistance for \emph{Staphylococcus aureus} and \emph{Mycobacterium tuberculosis}, matching the results of gold-standard methods in 99\% of drug-strain combinations for the former and >80\% for the latter.\autocite{Bradley2015} Both of these are admirable for being self-contained packages that anybody can download and run (Mykrobe can even run on standard macOS and Windows laptops) that require only one input—raw NGS reads in a format that all modern sequencers can produce (FASTQ) and produce a standard, actionable result. Neither, however, performs epidemiological analysis, i.e., assessment of multiple samples for the likelihood of transmission.

Our goal was to create a software suite to support the aims of the Pathogen Surveillance Program at The Mount Sinai Hospital, which applies long-read and short-read NGS technologies to routinely collected clinical microbiology specimens and aims to track and prevent the spread of healthcare associated infections (HAIs) throughout our health system. As this is a much broader goal than the aforementioned pipelines attempt, we broke the aims down into four modular, coordinated components: a LIMS suitable for genomic clinical microbiology (PathogenDB), an all-purpose bacterial assembly and annotation pipeline for the PacBio RSII (\pathogendbpipeline), a comparative genomics toolkit for the outputs of that pipeline (\pathogendbcomparison), and finally a visualization platform to turn these data into something clinically actionable (\pathogendbviz). In this chapter, we present the implementation of each of these components, the results enabled by the entire software suite to date, and our plan to disseminate the tools for broader use.

\begin{sidewaysfigure}[hp]
  \sidewaysvspace
  \centering
  \includegraphics[width=\textwidth]{chap4/pipeline-diag}
  \fullwidthlabelcaption{fig:pipeline_diag}{Overview of the PathogenDB suite}{
  \textbf{Overview of the PathogenDB suite}. The three modular processing components, \pathogendbpipeline, \pathogendbcomparison, and \pathogendbviz, surround our custom LIMS, PathogenDB, which serves as the central ``source of truth.'' The design of these components (and the circular path of information) directly reflects the workflow proposed in Figure \ref{fig:emr_ngs_loop}—note that the direction of flow in this diagram is generally counter-clockwise while it is clockwise in Figure \ref{fig:emr_ngs_loop}.
  }
\end{sidewaysfigure}

\section{Implementation}

A top-level view of all steps and their relationships is presented in Figure \ref{fig:pipeline_diag}. We will review the details of each major component (boxed segments) in sequence. Note that our current design and separation of concerns directly reflects the large circular workflow first presented in Figure \ref{fig:emr_ngs_loop}.

\subsection{Sample collection}

The first step of any genomic surveillance project is to collect and organize samples, which is reflected in the workflow represented at the upper left of Figure \ref{fig:emr_ngs_loop}. This is performed by people, not software. In the case of the Pathogen Surveillance Program, the Bakel lab receives daily deliveries of specimens from Mount Sinai's clinical microbiology laboratory. Staff in this lab must carry out a meticulous process of labeling, culturing, stocking, subcloning, and extracting DNA before any sequencing can occur.

However, to track the specimens, stocks, and derivative samples and associate them with metadata created during patient care (top center of Figure \ref{fig:emr_ngs_loop}), a database is necessary. In our workflow, we call this database PathogenDB, and it serves as the critical central ``source of truth'' for all operations and analyses. PathogenDB receives updates via online input forms from the staff in the Bakel lab creating stocks and performing cultures, subcloning, and DNA extraction. All of these items are barcoded and recorded in PathogenDB. The barcoding process (which involves assignment of a new ID) both serves to unqiuely identify every item and to remove any pre-existing association with patient identifiers. Simultaneously, PathogenDB receives automatic nightly reports from the EMR (Epic Systems) which contain all the isolates that were expected to be sent for that day along with clinical metadata like the hospital unit of collection, the collection date and time, the bodily source of the specimen (blood, stool, wound, etc.), and an opaque patient ID that does not correspond to any patient IDs reflected in the actual medical record (like medical record number). Only authorized staff, such as ID clinicians directly caring for patients and Infection Control physicians, are allowed to see the key linking patient metadata in PathogenDB back to outside medical records. This is done to ensure de-identification of Protected Health Information in accordance with the Safe Harbor Method principles.\footnote{See §164.514(b)(2) of the HIPAA Privacy Rule.}

\begin{figure}[htb]
  \includegraphics[width=\textwidth]{chap4/db-relations}               
  \caption[Entity-relationship diagram for the database underlying PathogenDB]{\textbf{Entity-relationship diagram for the database underlying PathogenDB,} using Information Engineering notation; see \textcite{Halpin2010}. Boxes represent tables of entities; single lines represent relationships, with arrowheads indicating the cardinality of each side of the relationship; crow’s foot arrowhead with circle represents “zero or more;” cross-stroke arrowhead represents “exactly one.”}
  \label{fig:pdb_relations}
\end{figure}

PathogenDB is implemented as a MySQL database, and the relational structure for core tables is depicted in Figure \ref{fig:pdb_relations}. The structure of the database mirrors the workflow of banking and preparing isolates, with steps proceeding roughly left to right starting from ``Isolates''—i.e., isolates can associated with one or more derivative stocks once culturing and banking are performed, which will then be associated with one or more extracts, which are then submitted to the Genomics Core Facilty at Mount Sinai (``sequencing core submissions''). The returned outputs from Genomics Core are logged as ``sequencing runs'' and then eventually ``assemblies'' are created upon completion of the \pathogendbpipeline (see next section). Maintaining a database for each step of the process ensures that items are not lost along the way, and that the provenance of every downstream product can be traced backwards, which can be crucial if contamination or mishandling are suspected.

\begin{sidewaysfigure}[hp]
  \sidewaysvspace
  \centering
  \includegraphics[width=\textwidth]{chap4/pdb-ui}               
  \fullwidthlabelcaption{fig:pdb_ui}{Overview of the web frontend for PathogenDB}{\textbf{Overview of the web frontend for PathogenDB.} A, the web portal permits quick access to basic views for all tables in the database, which can be searched, sorted, edited, and downloaded. At top, the Isolates table is shown, with certain potentially identifying information removed from the screenshot. At bottom, a zoomed view of the Assemblies table is shown, which the menu for switching between tables also shown. Links to other views (in B and C) are highlighted. B, sample quality control (QC) report for an assembly. Here, a graph of proportions of aligned reads that match various criteria is shown. Extreme fluctuations in these values can indicate assembly problems. C, ChromoZoom displaying a finished assembly, with annotated genes at top, two QC tracks in the center, and alignments of error corrected reads to the finished assembly at bottom. For more on ChromoZoom, see Chapter \ref{chap:chromozoom}.}
\end{sidewaysfigure}

PathogenDB provides a frontend that is based on phpMyEdit,\footnote{\url{http://www.phpmyedit.org/}} which is displayed in Figure \ref{fig:pdb_ui}. phpMyEdit allows for quick scaffolding of basic create-read-update-delete (CRUD) webpages for each of the tables in PathogenDB, which can be sorted, searched, edited, and downloaded by authorized members of the team. Authorization can be granted only to particular pages and views so that Bakel lab staff see only tables related to the isolate culturing and extraction workflow, while clinical coordinators instead see tables on patients due for sample collection and forms for sample submission.

Figure \ref{fig:pdb_ui}A shows a sample table view for the Isolates table, which tracks every biosample received by the Pathogen Surveillance Program. Isolates are associated with a Projects; if collected as part of routine operations, this is ``CML surveillance,'' but this field accommodates annotation of samples acquired from ad-hoc or outside investigations. Links at the far right of the table for Stocks and Minimum Inhibitory Concentrations (MICs) indicate that most Isolates are associated with two stocks and up to 14 measurements of antimicrobial susceptibility, which are extracted from Vitek (bioMérieux) results in the reports sent nightly by the EMR. Clicking on these links moves the user to a view of the corresponding entries in the related table. In the Assemblies table, shown at the bottom of this panel, special views are provided for viewing outputs of the \pathogendbpipeline{} (after assembly and annotation are complete). The first of these is a quality control report (Figure \ref{fig:pdb_ui}B), which shows plots of the final contig layout and read statistics, which can be useful for assessing trustworthiness of the assembly and diagnosing reasons for failure to circularize or high fragmentation. The second of these is a link to a ChromoZoom visualization of the genome (Figure \ref{fig:pdb_ui}C, also see Chapter \ref{chap:chromozoom}).

\subsection{\pathogendbpipeline: Assembly and annotation from raw long reads}

Once read data is available from the Genomics Core, computational analysis can begin. To date, the Pathogen Surveillance Program has chosen to sequence essentially all isolates on the PacBio RSII (see Chapter \ref{chap:steno} for methods). This sequencing platform comes with a manufacturer-maintained analysis toolkit called SMRT-Analysis,\footnote[][-2.5cm]{\url{https://github.com/PacificBiosciences/SMRT-Analysis}} which even includes a web interface (SMRT Portal), but we focus here on an automated solution.

\begin{figure*}[hb]
  \includegraphics[width=\textwidth]{chap4/pdb-pipeline}               
  \caption[Outline of steps automated by \pathogendbpipeline]{\textbf{Outline of steps automated by \pathogendbpipeline.} Processes are depicted as boxes, with processes requiring potentially multiple runs indicated as a ``stack.'' An interim file format is depicted as a single arrow, and groups of files as doubled arrows. The pipeline concludes with deposition of a link to an IGB Quickload Directory for the completed assembly into PathogenDB.}
  \label{fig:pdb_pipeline}
  \setfloatalignment{b}
\end{figure*}

The chosen strategy for converting PacBio RSII reads into a final annotated assembly is outlined in Figure \ref{fig:pdb_pipeline}. Briefly, the H5 outputs of the instrument (which contains movies of the single molecule reads) are assembled \emph{de novo} using the Hierarchical Genome Assembly Process\autocite{Chin2013} (HGAP), which is included in SMRT-Analysis. This produces a draft assembly, but since HGAP cannot create circular contigs, a FASTA of this assembly and the FASTQs of error-corrected reads produced by HGAP are passed off to \texttt{circlator},\autocite{Hunt2015} a recently released tool that automatizes \emph{circularization} by using SPAdes\autocite{Bankevich2012} to re-assemble the error-corrected reads that overlapped the ends of contigs. This produces a new assembly that is then \emph{polished} over the circularized junctions by re-mapping raw reads using BLASR\autocite{Chaisson2012} and recalling the consensus with Quiver,\autocite{Chin2013} which reduces errors in these regions by re-incorporating information that could not have aligned to them during the initial HGAP assembly. The polished assembly is then \emph{reoriented} back to the origin of replication (a convention for GenBank bacterial chromosome sequences) using a custom script\footnote{ \texttt{scripts/post\textunderscore quiver\textunderscore orient\textunderscore correct.py} in the \href{https://github.com/powerpak/pathogendb-pipeline/blob/master/scripts/post\textunderscore quiver\textunderscore orient\textunderscore correct.py}{GitHub repo}.} that performs a BLAST against \texttt{circlator}'s suggested origin point for each circular contig, which it decides based on a PROmer\autocite{Kurtz2004} search for \emph{dnaA} sequences.

This circularized, polished, and reoriented assembly is finally ready for annotation. Firstly, the contigs are renamed from the overly verbose HGAP and Quiver defaults. We use an in-house convention of starting all contig names with ``\texttt{u}'' (for ``unitig'', a term from Celera\footnote{See \url{http://wgs-assembler.sourceforge.net/wiki/index.php/Celera_Assembler_Terminology}}) followed by a five-digit unitig number originally assigned by HGAP. This is followed by three letters that flag for \underline{c}ircularization, \underline{r}eorientation, \underline{p}olishing, and an additional letter reserved for later use, with ``\texttt{x}'' indicating failure of that step. Another letter surrounded by underscores signals the hypothesized type of contig (\underline{c}hromosome, \underline{p}lasmid, \underline{m}erged, \underline{g}arbage, or \underline{o}ther).\footnote{We use the simple heuristic that any successfully circularized contig >1Mbp is a chromosome, and anything smaller is a plasmid. Merged and garbage contigs are the result of ambiguities during assembly. For more detail, see \href{https://github.com/powerpak/pathogendb-pipeline/blob/master/scripts/post\textunderscore circlator\textunderscore contig\textunderscore rename.py}{\texttt{scripts/post\textunderscore circlator\textunderscore contig\textunderscore rename.py}}.} Finally, the original SMRT-analysis job number set by the Genomics Core is appended. This renaming results in a contig ID like \verb|u00011crpx_p_023011|, indicating it is the 11th unitig, was \underline{c}ircularized, \underline{r}eoriented, and \underline{p}olished, is probably a \underline{p}lasmid, and came from sequencing job \#023011. These names are short enough for easy viewing in downstream tools like ChromoZoom, while retaining as much information as possible about the provenance and assembly status of the contig. Renamed contigs are finally annotated with \texttt{prokka},\autocite{Seemann2014} which completely encapsulates detection of putative coding regions and maps them to annotated gene names in UniProt.\autocite{Wasmuth2016} We then run a series of custom scripts to generate diagnostic files for the QC website (Figure \ref{fig:pdb_ui}B) and the assembly and related tracks into an IGB Quickload Directory that can be loaded into ChromoZoom (Figure \ref{fig:pdb_ui}C).

\pathogendbpipeline{} is implemented as a \texttt{Rakefile}, which is written in Ruby and executed by \texttt{rake}, an analog of GNU \texttt{make}. GNU \texttt{make} was originally written as a build system for automating the generation of executables and other products from a program's source files. The advantage of a build system is that it encourages the explicit annotation of dependencies between interim files and tasks into the pipeline, thereby allowing for previous products of a partial build to be automatically re-used if their dependencies (source files) have not changed. From the user's perspective, another benefit is that only the desired final task in the pipeline needs to be specified, and \texttt{rake} can automatically figure out what preceding takss are required to get to that point. For most runs, the sequence of tasks is as follows:

\begin{enumerate}[label=\arabic*.,noitemsep,labelindent=2em,leftmargin=!]
\item \verb|pull_down_raw_reads|
\item \verb|assemble_raw_reads|
\item \verb|run_circlator|
\item \verb|post_circlator|
\item \verb|resequence_assembly|
\item \verb|post_quiver_orient_correct|
\item \verb|prokka_annotate|
\item \verb|create_QC_webpage|
\item \verb|prokka_to_igb|
\end{enumerate}

which mostly correspond, unsurprisingly, to the boxes in Figure \ref{fig:pdb_pipeline}. Most of these tasks require some arguments, such as the expected species or the destination for output. These are specified as environment variables during execution, so to get to the final \verb|prokka_to_igb| step above, the user would run something like:

\begin{verbatim}
    $ rake OUT=scratch/out/ER05681 \
           SMRT_JOB_ID=023154 \
           STRAIN_NAME=ER05681 \
           SPECIES="Staphylococcus aureus" \
           prokka_to_igb
\end{verbatim}

If problems occur during assembly, the pipeline supports manual editing of the interim FASTA files, and then a flag (\texttt{CURATED=1}) can be set to signify that manual curation occurred and to therefore bypass circularization, reorientation, and contig renaming. There is also an optional branch (bottom entry point for Figure \ref{fig:pdb_pipeline}) that can incorporate Illumina short reads during assembly. Auxiliary sequencing on a short read platform can correct small errors that HGAP on PacBio reads will miss—typically indels in homopolymeric stretches. For this branch, \texttt{bwa}\autocite{Li2010b} is used to perform in-memory alignment against the circularized, reoriented, and polished assembly, and \texttt{samtools}\autocite{Li2009b} and \texttt{vcftools}\autocite{Danecek2011a} are used to call variants from the pileup and create a new FASTA consensus. These steps can in fact be repeated several times (crossover loop in Figure \ref{fig:pdb_pipeline}) to call a progressively more accurate consensus, although this decision must currently be made manually.\footnote{A future version of the pipeline might attempt to re-run the steps until the consensus stabilizes.}

The modularity of siloing tasks in a \texttt{Rakefile} provided long-term advantages besides those mentioned previously. In our case, when we first created the pipeline in 2013, mature tools for some of the steps did not yet exist, e.g., \texttt{circlator} and \texttt{prokka} were not yet publicly available. Therefore, we used less than optimal solutions, such as our own custom script for circularization and the RAST web service\autocite{Aziz2008} for annotation, as detailed in Chapter \ref{chap:steno}. Once these new tools became available, it was relatively simple to swap them into the pipeline while preserving the old code under an \texttt{old:} namespace, indicating deprecated tasks. Because the tasks and their dependencies are relatively isolated, different members of the team could create slightly different versions of certain interim steps while still sharing all code in a common \texttt{Rakefile}. This was of course further reinforced by keeping all code under version control.

\subsection{\pathogendbcomparison: Comparative analysis on complete genomes}

After deposition of a complete annotated assembly's IGB Quickload Directory and corresponding record into the Assemblies table of PathogenDB, the next step of analysis is to compare all genomes for a given species (perhaps limited by timeframe or location) against one another to determine relatedness and the likelihood of transmissions. We implemented this as the next component in our suite, called \pathogendbcomparison.

\begin{figure*}[htb]
  \includegraphics[width=0.8\textwidth]{chap4/pdb-comparison}               
  \caption[Outline of steps automated by \pathogendbcomparison]{\textbf{Outline of steps automated by \pathogendbcomparison.} Processes are depicted as boxes, with processes requiring potentially multiple runs indicated as a ``stack.'' An interim file format is depicted as a single arrow, and groups of files as doubled arrows. The pipeline concludes with various outputs being sent to \pathogendbviz for further visualization.}
  \label{fig:pdb_comparison}
\end{figure*}

Our implementation of workflows for this stage of analysis is outlined in Figure \ref{fig:pdb_comparison}. As emphasized previously, PathogenDB is considered the single source of ``truth'' from which all assemblies and metadata are queried before running an analysis; however, some of the tasks are generic enough to run on an arbitrary set of FASTA files without metadata. Like \pathogendbpipeline, \pathogendbcomparison is also implemented using \texttt{rake}, but its workflow is more branched. The three types of implemented analyses, reflected in Figure \ref{fig:pdb_comparison} by the three arrows emerging from the \texttt{rake} box and listed here with their corresponding \texttt{rake} task name, are:

\begin{enumerate}[label=\arabic*.,noitemsep,labelindent=2em,leftmargin=!]
\item \verb|mauve|: Mauve alignment, which highlights structural variants
\item \verb|snv|: Pairwise MUMmer single nucleotide variant (SNV) distances for heatmap visualization
\item \verb|mugsy|: Core genome alignment for a phylogeny with branch lengths scaled to SNV distances
\end{enumerate}

A Mauve alignment of \emph{S. maltophila} genomes was previously depicted in Figure \ref{fig:mauve} and is most useful for finding large insertions, deletions, translocations, and other recombinatorial events. Mauve performs alignments by using an anchoring heuristic to search for large areas of homology among subsets of the input genomes, which it terms local collinearity blocks (LCBs).\autocite{Darling2010} Our corresponding task simply wraps execution of \texttt{progressiveMauve}\autocite{Darling2010} and returns the XMFA alignment, since visualization is typically performed with the Mauve Java application. However, we also provide a task that can calculate pairwise rearrangement distances using GRIMM,\autocite{Tesler2002} which searches for the minimal number of inversion operations needed to transform one genome's sequence of LCBs into the other genome's. Because inversion distance is algorithmically simple to calculate\autocite{Hannenhalli1999} but probably reflects evolutionary edit distances less accurately than newer models like double-cut and join (DCJ),\autocite{Lin2008} we have not yet made full use of these distances, but hope to eventually incorporate a wide array of structural variant edit distances into downstream analysis.\autocite{Hilker2012}

MUMmer is a versatile software suite for fast pairwise genome alignment that finds maximally exact matches (MEMs, formerly maximal unique matches, hence MUM) usin a suffix tree algorithm that can run in linear time.\autocite{Kurtz2004} Although it is excellent for finding subsequences of one genome that are within a certain edit distance of all locations on another genome, because of the strict edit distance threshold it is less suited for finding large rearrangements when compared to Mauve. However, it is very well suited for quickly calling SNVs between two genomes, as long as the SNVs are not so closely spaced as to elude a match (which should occur only extremely rarely). For this task, we use the \texttt{show-snps} tool within MUMmer to call SNVs between all pairs of input genomes, which produces a distance matrix that can be visualized with \pathogendbviz.

The same strategy is also used to rescale branches for our phylogenetic analysis, which we perform by creating a core genome alignment with \texttt{mugsy},\autocite{Angiuoli2011a} a multiple genome aligner that internally combines MUMmer and its own algorithm for finding LCBs.\footnote{Recently, the Harvest suite was released for core genome alignment, which scales better to thousands of genomes than \texttt{mugsy}, and it even includes its own visualization tool, \texttt{gingr}. We are currently in the process of incorporating these tools into our pipeline. For more, see \textcite{Treangen2014}} The core genome alignment in MAF format is converted to PHYLIP format with CLUSTALW,\autocite{Sievers2011} and then this alignment undergoes maximum-likelihood phylogenetic inference via RAxML.\autocite{Stamatakis2005} Since the outputted tree (in Newick format) initially has distances in units of time under the RAxML evolutionary model, which is more opaque than SNV distance, the tree's branches are rescaled by recalculating SNV distance using the aforementioned \texttt{show-snps} method on all adjoining nodes, including the ancestral states imputed by RAxML. Phylograms of these with overlaid SNV distances (as in Figure \ref{fig:steno_phylo}) can be plotted to PDFs using an included \verb|mugsy_plot| task that wraps the \texttt{ape} R package.

Although maximum likelihood phylogenetic analysis is a standard component of molecular epidemiology and typically the centerpiece of most published investigations of outbreaks using NGS,\autocite{Azarian2015,Eyre2012,Joensen2014,Casali2016} it may in fact be overkill for answering the simpler day-to-day question of ``are any new isolates closely related to the previously sequenced isolates?'' By design, it requires a core genome alignment for all of the genomes that one wishes to include in the tree. Although every multiple sequence aligner uses heuristics to save time, multiple sequence alignment has a fundamental algorithmic complexity of $O(g^n)$ under the usual dynamic programming approaches,\autocite{Just2004} where $g$ is the average length of a genome and $n$ is the number of genomes—i.e., exponential to the number of genomes. Even though tools for core genome alignment continue to get smarter and faster about subverting this complexity,\autocite{Treangen2014} given the fundamental difficulties in scaling that problem, we anticipate that pairwise distance matrices will be a suitable alternative for outbreak detection as databases of thousands of assembled sequences become commonplace. Creating a distance matrix is guaranteed to be $O(n^2)$ in the most naive approach,\footnote{And this is how it is currently implemented; precalculating the MLST for all isolates and only allowing within-MLST comparisons would be a simple first optimization.} with each pairwise comparison being $O(g)$ by use of \texttt{show-snps}. Furthermore, adding one new assembled isolate does not require recalculating everything as in multiple sequence alignment, but we can instead add one row and column to the existing matrix in $O(2n)$ time. For these reasons we provide the \verb|snv| task in conjunction with \verb|mugsy|, and we rely on the distance matrices for analyses on >100 genomes, as presented later in the Results.

\subsection{\pathogendbviz}

Finally, visualization is performed with the \pathogendbviz{} toolkit. The interface, which has a heatmap layout and a geospatial layout, will be presented in the Results in Figure FIXME. \pathogendbviz{} reads data from JSON files containing genetic distances and isolate metadata as prepared by \pathogendbcomparison{}'s \verb|heatmap| task, and displays it in a HTML5 interface that draws data dynamically to scalable vector graphics (SVG) using the d3.js Javascript library.\footnote{\url{https://d3js.org/}} \pathogendbviz{} is currently implemented as a single PHP page that loads all data via Asynchronous Javascript and XML (AJAX).\autocite{Paulson2005} Given that all data is drawn on the client side, it attempts to maximize the control the user has over the selection of isolates to displayed and how the comparison is presented. Agglomerative hierarchical clustering is performed in the browser using the \texttt{ml-hclust}\footnote{\url{https://www.npmjs.com/package/ml-hclust}} node.js package, using single linkage to emphasize ``chaining'' of closely related genomes. The heatmap.js\footnote{\url{https://www.patrick-wied.at/static/heatmapjs/}} library is used to draw density plots of epidemiological incidence in the geospatial layout. Sequenced isolates are displayed in the geospatial layout as a live-updating force-directed network using \texttt{d3.forceSimulation}, with a strong force keeping nodes from colliding, a moderate force pulling nodes toward the position of specimen collection, and a very weak spring force along the edges (which represent putative transmissions under the selected SNV threshold).

\subsection{Availability}

Source code for \pathogendbpipeline, \pathogendbcomparison, and \pathogendbviz{} is publicly available from GitHub at:

\begin{enumerate}[label=\arabic*.,noitemsep,labelindent=2em,leftmargin=!]
\item \url{https://github.com/powerpak/pathogendb-pipeline}
\item \url{https://github.com/powerpak/pathogendb-comparison}
\item \url{https://github.com/powerpak/pathogendb-viz}
\end{enumerate}

The code in each repository is still under active development to suit the operational needs of the Pathogen Surveillance Program. The software is currently configured for execution on Mount Sinai's high performance computing environment (Minerva), but we will adapt it for simple installation on vanilla Linux distributions and provide machine images suitable for common cloud computing environments once we are ready to promote its wider usage.

\section{Results and Discussion}

\subsection{Assembly quality}

As of April 2017, \pathogendbpipeline{} has been used to assemble and annotate 593 genomes from 7 species. General statistics on assemblies produced by the pipeline are presented in Table \ref{tab:pathogendb_assemblies}. Most of the isolates assembled so far are \emph{Staphylococcus aureus} and \emph{Clostridium difficile}. About three quarters of assembled \emph{S. aureus} isolates were methicillin-resistant (MRSA). A few other rarer species have also been assembled.\footnote{The \emph{S. maltophilia} isolates from Chapter \ref{chap:steno} are not included in this table.} 164 assemblies underwent manual curation during the development of the pipeline and are excluded from statistics on assembly quality.

\begin{table}[htb]
  \input{chap/tables/pathogendb-assemblies}
  \caption[Statistics on assemblies generated by \pathogendbpipeline{} since 2013]{\textbf{Statistics on assemblies generated by \pathogendbpipeline{} since 2013.} $^a$Any contig ≥1Mbp that circularized was considered a chromosome. Abbreviations: N50, shortest contig length above which 50\% of the genome is included; Mbp, 1 million base pairs; kbp, 1 thousand base pairs.
}
  \label{tab:pathogendb_assemblies}
\end{table}

Without curation, \pathogendbpipeline{} is able to completely assemble most of the sequenced isolates, with >70\% featuring a circular main chromosome. The \emph{N50}, defined as the shortest contig at which it and all larger contigs would include 50\% of the genome, was ≥1Mbp for 96.0\% of uncurated genomes. 80.4\% of completed assemblies contained four or fewer contigs, which allows for one chromosome and up to three plasmids or unassembled fragments. From these metrics, \pathogendbpipeline{} is clearly able to produce high-quality \emph{de novo} genome assemblies with little human intervention.

\subsection{Computational benchmarks for \pathogendbpipeline}

\begin{figure*}[htb]
  \includegraphics[width=\textwidth]{chap4/pdb-benchmark}               
  \caption[Boxplots of computational benchmarks for \pathogendbpipeline]{\textbf{Boxplots of computational benchmarks for \pathogendbpipeline{} on a \emph{S. aureus} and a \emph{C. difficile} isolate.} For each isolate, measurements were collected from 10 end-to-end serial runs of the pipeline, starting at raw read data and ending at the \texttt{prokka\textunderscore to\textunderscore igb} task, using a single server with a 12-core Intel Xeon(R) 2.5GHz E5-2680 CPU and 128GB of RAM.}
  \label{fig:pdb_benchmark}
\end{figure*}

Although over the past four years of development its end-to-end time has dramatically dropped, \pathogendbpipeline{} is still by far the most computationally intensive module within the PathogenDB suite. The majority of the cost is accrued during the \emph{de novo} assembly and polishing steps, which require stepping through all read data for each sequenced isolate, which commonly exceeds 1Gbp per isolate. In Figure \ref{fig:pdb_benchmark} we provide benchmarks for the impact of \pathogendbpipeline{} on overall turnaround time for an end-to-end analysis and the corresponding computational cost. We performed 10 serial end-to-end runs of the pipeline starting from raw read data for one \emph{S. aureus} isolate and one \emph{C. difficile} isolate on a 12-core Intel Xeon(R) 2.5GHz E5-2680 server with 128GB of RAM. Most of the runs produced nearly identical benchmarks, so the interquartile ranges in Figure \ref{fig:pdb_benchmark} are very narrow. The median walltime (which measures real-world start to end time) was under two hours, with none of the runs exceeding three hours (Figure \ref{fig:pdb_benchmark}A). The jobs benefited from multicore usage, as the median user CPU time exceeded the walltime by a factor of 4-5× (Figure \ref{fig:pdb_benchmark}B), and this is confirmed by checking average CPU usage, which is normalized against a single core and generally in the 400-600\% range (Figure \ref{fig:pdb_benchmark}C). Although HGAP, BLASR, and Quiver are memory-intensive steps, the total memory used did not exceed 7GB for any of the runs (Figure \ref{fig:pdb_benchmark}D).

\subsection{Clinically actionable visualizations}

After completing assembly and annotation for a large set of same-species isolates, \pathogendbcomparison{} can produce comparative analyses that are viewable with \pathogendbviz. Figure \ref{fig:pdb_heatmap_saureus} displays the interface of \pathogendbviz{} for a heatmap visualization of all \emph{S. aureus} isolates with ≥1 closely related isolate from another patient. The interface allows a clinican or infection control officer to quickly ``drill down'' to the data of interest. The top left displays a dropdown with different datasets, all created by \pathogendbcomparison. The top right has a slider used to set the threshold in SNVs (as detected by MUMmer) for considering two isolates to be related enough for putative transmission.

\begin{sidewaysfigure}[hp]
  \sidewaysvspace
  \centering
  \includegraphics[width=0.85\textwidth]{chap4/pdb-heatmap-saureus}               
  \fullwidthlabelcaption{fig:pdb_heatmap_saureus}{\pathogendbviz{} heatmap visualization for all putatively transmitted \emph{S. aureus} isolates, based on NGS}{\textbf{\pathogendbviz{} heatmap visualization for all putatively transmitted \emph{S. aureus} isolates, based on NGS.} Note that dates have been shifted into the future and unit names have been obfuscated to eliminate any potentially identifying information. At top, user controls allow selection of the dataset, SNV threshold, merging and prefiltering of isolates, and ordering of the diagram. A horizontal beeswarm shows the distribution of isolates over collection times, which the user can ``brush'' to select specific time ranges. At adjacent right, a histogram of SNV distances between each isolate and its closest neighbor helps inform what a reasonable SNV thershold might be; in black, isolates from the same patient (which are expected to be related); in gray, isolates from any previous patient. At bottom left, a clustered heatmap depicts distances between isolates that exceed the SNV threshold as big colored blocks along the diagonal. At bottom right, a hierarchical clustering shows the SNV distance between rows in the heatmap.}
\end{sidewaysfigure}

FIXME: finish

\section{Conclusion}

FIXME: finish
